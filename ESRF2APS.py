import h5py
import argparse
import numpy as np
import os

def data_scope(assigned_title):
	"""
		Extracts relevant data scope from the assigned title.

		Parameters:
		assigned_title (str): Title assigned to a dataset.

		Returns:
		str: Comma-separated list of matching keywords in the title, or 0 if no matches found.
	"""
	keywords = [b'dark', b'flat', b'projections']  # Use bytes-like objects for keywords
	matching_keywords = [keyword.decode('utf-8') for keyword in keywords if keyword in assigned_title.lower()]
	return ', '.join(matching_keywords) if matching_keywords else None


def ReadDatasets(group, datasets, param):
	"""
		Recursively reads datasets within an HDF5 group, identifies 3D arrays, and extracts relevant information.

		Parameters:
		group (h5py.Group): HDF5 group to explore.
		datasets (list): List to store identified datasets along with their associated group and type.
		param (dict): Dictionary to store additional parameters like scan_range, half_acquisition, and npoints.
	"""

	esrf_meta = {
		'scan_range': 'scan_range',
		'half_acquisition': 'half',
		'npoints': 'angles'
	}
	for name, item in group.items():
		if isinstance(item, h5py.Group):
			# Recursively search within groups
			ReadDatasets(item, datasets, param)

		elif isinstance(item, h5py.Dataset) and item.ndim == 3 and name == 'image':
			path = '/' + str(group).split('/')[1] + '/title'
			try:
				title_dataset = group.file[path]
				typeD = data_scope(title_dataset[()])
				if typeD:
		    			datasets.append((item, group, typeD))
			except KeyError:
				print(f"Dataset '{path}' not found.")
		elif name in esrf_meta:#== 'scan_range' or name == 'half_acquisition' or name == 'npoints':
			param[name] = item[()]


def read_linked_files(dataset):
	"""
		Reads linked files from a virtual dataset.

		Parameters:
		- dataset (h5py.VirtualDataset): Input virtual dataset.

		Returns:
		- list of str: List of linked file names.
	"""
	virtual_sources = dataset.virtual_sources()
	linked_files = []
	for source in virtual_sources:
		linked_files.append(source.file_name)
	return linked_files

def assign_data(data_path, group_path, dataset, output_file, key):
	"""
		Assigns virtual datasets to an output HDF5 file based on linked files.

		Parameters:
		- data_path (str): Path to the directory containing linked HDF5 files.
		- group_path (str): Path to the group within the linked HDF5 files.
		- dataset (numpy.ndarray): Input dataset to be linked.
		- output_file (h5py.File): HDF5 file where the virtual dataset will be created.
		- key (str): Key for the virtual dataset in the output file.
	"""
	# Read linked files from the dataset
	linked_files = read_linked_files(dataset)

	# Define the group path within the HDF5 file
	gpath = '/entry_0000/' + group_path + '/data'

	# Create a virtual layout for the output dataset
	layout = h5py.VirtualLayout(dataset.shape, dtype=float)
	if key != '/exchange/data' and len(linked_files)>1:
		#Removes averaged whites
		file_path = os.path.join(data_path, linked_files[0])
		# Create a virtual source for the current file
		vsource = h5py.VirtualSource(file_path, gpath, shape=(int(dataset.shape[0]/len(linked_files)), dataset.shape[1], dataset.shape[2]))
		layout[:int(dataset.shape[0]/len(linked_files)), :, :] = vsource
	else:			
		# Iterate over each HDF5 file
		for n, hdf5_file in enumerate(linked_files):
			# Create the full path to the HDF5 file
			file_path = os.path.join(data_path, hdf5_file)
			# Create a virtual source for the current file
			vsource = h5py.VirtualSource(file_path, gpath, shape=(int(dataset.shape[0]/len(linked_files)), dataset.shape[1], dataset.shape[2]))
			
			# Assign the virtual source to the layout
			layout[n*int(dataset.shape[0]/len(linked_files)):(n+1)*int(dataset.shape[0]/len(linked_files)), :, :] = vsource

		# Create a virtual dataset in the output file linked to the virtual layout
	output_file.create_virtual_dataset(key, layout)



def APS_format(input_file, new_file_path):
	"""
		Converts ESRF to APS IMG data structure.

		Parameters:
		input_file (str): Path to the existing HDF5 file (ESRF format).
		new_file_path (str): Path to the new HDF5 file (APS IMG format).
	"""
	data_path = path_only = os.path.dirname(input_file)
	
	with h5py.File(input_file, 'r') as input_file, h5py.File(new_file_path, 'w') as outfile:
		# Create groups
		APS_base = '/exchange/'
		
		data_g = outfile.create_group(APS_base)
		defa = outfile.create_group('/default')
		meas = outfile.create_group('/measurement')
		proc = outfile.create_group('/process')

		# Use the previous function to identify 3D arrays and 'title' entries
		datasets_list = []
		param = {}
		ReadDatasets(input_file, datasets_list, param)
		data_map = {
		    'projections': 'data',
		    'flat': 'data_white',
		    'dark': 'data_dark'
		}
		for dataset, group, measurement in datasets_list:
			target_path = dataset.name
			group_path = '/'.join(group.name.split('/')[2:])
			
			key = data_map.get(measurement, None)
			if key:
				try:
					key = f'{key}F' if key == 'data_white' and 'data_white' in data_g else key
					assign_data(data_path, group_path, dataset, outfile, APS_base + key)
				except ValueError:
					pass
		# Create angle array
		data_g.create_dataset(f'theta', data=np.linspace(0, int(param['scan_range']), int(param['npoints'])))

if __name__ == "__main__":
	parser = argparse.ArgumentParser(description='Convert ESRF to APS IMG data structure.')
	parser.add_argument('-i', dest='fname', required=True, help='Path to the existing HDF5 file.')
	parser.add_argument('-o', dest='outname', required=True, help='Path to the new HDF5 file.')

	args = parser.parse_args()
	APS_format(args.fname, args.outname)
